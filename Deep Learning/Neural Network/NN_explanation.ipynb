{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x1 = tf.Variable([2.0], dtype=tf.float64, name='x1')\n",
    "w1 = tf.Variable([-3.0], dtype=tf.float64, name='w1')\n",
    "x2 = tf.Variable([0.0], dtype=tf.float64, name='x2')\n",
    "w2 = tf.Variable([1.0], dtype=tf.float64, name='w2')\n",
    "b = tf.Variable([6.8813], dtype=tf.float64, name='bias')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    o = tf.tanh((x1 * w1) + (x2*w2) + b)\n",
    "grads = tape.gradient(o, [x1, w1, x2, w2])\n",
    "\n",
    "print(o.numpy().item())\n",
    "print(\"----------------\")\n",
    "print(x1.name, grads[0].numpy().item())\n",
    "print(w1.name, grads[1].numpy().item())\n",
    "print(x2.name, grads[2].numpy().item())\n",
    "print(w2.name, grads[3].numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(17, 5))\n",
    "fig.suptitle(f'Activation functions', fontsize=16)\n",
    "x = np.linspace(-10, 10, 100)\n",
    "ax = axes[0]\n",
    "ax.plot(x, np.tanh(x))\n",
    "ax.set_title('Tanh')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x, np.maximum(0, x))\n",
    "ax.set_title('ReLu')\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(x, 1/(1 + np.exp(-x)) )\n",
    "ax.set_title('Sigmoid')\n",
    "\n",
    "ax = axes[3]\n",
    "ax.plot(x, x)\n",
    "ax.set_title('Linear')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
