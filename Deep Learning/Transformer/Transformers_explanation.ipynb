{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedings\n",
    "Embeddings are a way to represent words in a way that a machine learning model can understand. They are a way to convert words into numbers. In this notebook, we will see how to use embeddings in a machine learning model. We will use the tf.keras.layer.Embedding for this purpose.\n",
    "Maybe later I will create my own embeddings using the word2vec algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x17f1c7f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Embedding(10, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encoding(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(positional_encoding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        positions = tf.range(input_shape[1], dtype=tf.float32)[:, tf.newaxis]\n",
    "        dim = tf.range(input_shape[-1], dtype=tf.float32)[tf.newaxis, :]\n",
    "        self.pe = positions / tf.pow(10000, 2 * (dim // 2) / input_shape[-1])\n",
    "        self.pe = tf.where( tf.cast(dim % 2, tf.bool),tf.cos(self.pe), tf.sin(self.pe))\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.pe + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention(Layer):\n",
    "    def __init__(self, heads=8):\n",
    "        super(multi_head_attention, self).__init__()\n",
    "        self.Q_initializer = GlorotUniform()\n",
    "        self.K_initializer = GlorotUniform()\n",
    "        self.V_initializer = GlorotUniform()\n",
    "        self.WO_initializer = GlorotUniform()\n",
    "        self.heads = heads\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.n_dims = input_shape[-1]\n",
    "        self.WQ = self.add_weight(shape=(self.n_dims, self.n_dims), initializer=self.Q_initializer, trainable=True)\n",
    "        self.WK = self.add_weight(shape=(self.n_dims, self.n_dims), initializer=self.K_initializer, trainable=True)\n",
    "        self.WV = self.add_weight(shape=(self.n_dims, self.n_dims), initializer=self.V_initializer, trainable=True)\n",
    "        self.WO = self.add_weight(shape=(self.n_dims, self.n_dims), initializer= self.WO_initializer, trainable=True)\n",
    "    \n",
    "    def call(self,x):\n",
    "        ### x shape = (batch_size, seq_len, n_dims)\n",
    "        Q = x @ self.WQ\n",
    "        K = x @ self.WK\n",
    "        V = x @ self.WV\n",
    "\n",
    "        ### Splitting the heads and stacking them\n",
    "        Q = tf.stack(tf.split(Q, self.heads, axis=2))\n",
    "        K = tf.stack(tf.split(K, self.heads, axis=2))\n",
    "        V = tf.stack(tf.split(V, self.heads, axis=2))\n",
    "        ### Applying the attention\n",
    "        return self.attention(Q, K, V) @ self.WO\n",
    "\n",
    "    def attention(self, Q, K, V):\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = softmax(QK / np.sqrt(self.n_dims), axis=-1)\n",
    "        QKV = tf.matmul(QK, V)\n",
    "        return tf.concat(tf.unstack(QKV, axis=0), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_n_norm(Layer):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super(add_n_norm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.n_dims = input_shape[-1]\n",
    "        self.gamma = self.add_weight(shape=(self.n_dims,), initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(self.n_dims,), initializer='zeros', trainable=True)\n",
    "    \n",
    "    def call(self, x, x_i):\n",
    "        ## x_i is the input to the sublayer\n",
    "        ## x is the output from the sublayer\n",
    "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.math.reduce_std(x, axis=-1, keepdims=True)\n",
    "        normalized_values = (x - mean) / tf.sqrt(tf.square(std) + self.epsilon) * self.gamma + self.beta\n",
    "        return normalized_values + x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_layer(Layer):\n",
    "    def __init__(self, n_out, activation):\n",
    "        super().__init__(name=f'Neuron')\n",
    "        self.w_initializer = GlorotUniform()\n",
    "        self.b_initializer = GlorotUniform()\n",
    "        self.n_out = n_out\n",
    "        self.activation = activation\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=[input_shape[-1],self.n_out], initializer=self.w_initializer, trainable=True)\n",
    "        self.b = self.add_weight(shape=[self.n_out,], initializer=self.b_initializer, trainable=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        z = x @ self.w + self.b\n",
    "        if self.activation:\n",
    "            z =  self.activation(z)\n",
    "        return z     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Multi-head Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class masked_multi_head_attention(Layer):\n",
    "    def __init__(self, heads=8):\n",
    "        super(masked_multi_head_attention, self).__init__()\n",
    "        self.Q_initializer = GlorotUniform()\n",
    "        self.K_initializer = GlorotUniform()\n",
    "        self.V_initializer = GlorotUniform()\n",
    "        self.WO_initializer = GlorotUniform()\n",
    "        self.heads = heads\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.n_dims = input_shape[-1]\n",
    "        self.WQ = self.add_weight(shape=(self.n_dims, self.n_dims), initializer=self.Q_initializer, trainable=True)\n",
    "        self.WK = self.add_weight(shape=(self.n_dims, self.n_dims), initializer=self.K_initializer, trainable=True)\n",
    "        self.WV = self.add_weight(shape=(self.n_dims, self.n_dims), initializer=self.V_initializer, trainable=True)\n",
    "        self.WO = self.add_weight(shape=(self.n_dims, self.n_dims), initializer= self.WO_initializer, trainable=True)\n",
    "        a = tf.linalg.band_part(tf.ones(shape= (input_shape[1], input_shape[1])), -1, 0)\n",
    "        mask = tf.not_equal(a, 1)\n",
    "        a = tf.where(mask, np.inf * -1, a)\n",
    "        mask = tf.greater(a, 0)\n",
    "        self.a = tf.where(mask, 0, a)\n",
    "\n",
    "    def call(self,x,Q):\n",
    "        ### x shape = (batch_size, seq_len, n_dims)\n",
    "        Q = Q @ self.WQ\n",
    "        K = x @ self.WK\n",
    "        V = x @ self.WV\n",
    "        \n",
    "        ### Splitting the heads and stacking them\n",
    "        Q = tf.stack(tf.split(Q, self.heads, axis=2))\n",
    "        K = tf.stack(tf.split(K, self.heads, axis=2))\n",
    "        V = tf.stack(tf.split(V, self.heads, axis=2))\n",
    "        ### Applying the attention\n",
    "        return self.attention(Q, K, V) @ self.WO\n",
    "\n",
    "    def attention(self, Q, K, V):\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK + self.a   ### Masking the attention\n",
    "        QK = softmax(QK / np.sqrt(self.n_dims), axis=-1)\n",
    "        QKV = tf.matmul(QK, V)\n",
    "        return tf.concat(tf.unstack(QKV, axis=0), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(Layer):\n",
    "    def __init__(self, n_out):\n",
    "        super(linear, self).__init__()\n",
    "        self.n_out = n_out\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.n_out), initializer='ones', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        return softmax(x @ self.w, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(Layer):\n",
    "    def __init__(self, n_heads = 8):\n",
    "        super(encoder_layer, self).__init__()\n",
    "        self.mha = multi_head_attention(n_heads)\n",
    "        self.add_norm1 = add_n_norm()\n",
    "        self.dense = dense_layer(2048, tf.nn.relu)\n",
    "        self.dense1 = dense_layer(512, None)\n",
    "        self.add_norm2 = add_n_norm()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x1 = self.mha(x)\n",
    "        x = self.add_norm1(x1, x)\n",
    "        x1 = self.dense(x)\n",
    "        x1 = self.dense1(x1)\n",
    "        x = self.add_norm2(x, x1)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(Layer):\n",
    "    def __init__(self, n_layers=6, n_heads=8, input_dims = 6 ,output_dims=512):\n",
    "        super(encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dims, output_dims)\n",
    "        self.pe = positional_encoding()\n",
    "        self.layers = [encoder_layer(n_heads= n_heads) for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_layer(Layer):\n",
    "    def __init__(self, n_heads = 8):\n",
    "        super(decoder_layer, self).__init__()\n",
    "        self.mha1 = masked_multi_head_attention(n_heads)\n",
    "        self.add_norm1 = add_n_norm()\n",
    "        self.mha2 = multi_head_attention(n_heads)\n",
    "        self.add_norm2 = add_n_norm()\n",
    "        self.dense = dense_layer(2048, tf.nn.relu)\n",
    "        self.dense1 = dense_layer(512, None)\n",
    "        self.add_norm3 = add_n_norm()\n",
    "\n",
    "    \n",
    "    def call(self, x, enc):\n",
    "        x1 = self.mha1(x, enc)\n",
    "        x = self.add_norm1(x1, x)\n",
    "        x1 = self.mha2(x)\n",
    "        x = self.add_norm2(x, x1)\n",
    "        x1 = self.dense(x)\n",
    "        x1 = self.dense1(x1)\n",
    "        x = self.add_norm3(x, x1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(Layer):\n",
    "    def __init__(self, n_layers=6, n_heads=8, output_dims=512, input_dims=6, tokens = 1000):\n",
    "        super(decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dims, output_dims)\n",
    "        self.pe = positional_encoding()\n",
    "        self.layers = [decoder_layer(n_heads= n_heads) for _ in range(n_layers)]\n",
    "        self.linear = linear(tokens)\n",
    "\n",
    "\n",
    "    def call(self, x, enc):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "For this transformer I did not use the linear transformation layer, because I am trying to predict the secondary structure of a RNA. Since the output is a adjacency matrix, I did not see the need to use the linear transformation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, input_dims=6, output_dims=6, n_layers=6, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder(n_layers= n_layers, n_heads= n_heads, input_dims=input_dims)\n",
    "        self.decoder = decoder(n_layers= n_layers, n_heads= n_heads, input_dims=input_dims, tokens= output_dims)\n",
    "\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(y, x)\n",
    "        return x\n",
    "    \n",
    "    def compile(self, optimizer, loss):\n",
    "        super(Transformer, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, y)\n",
    "            loss = self.loss(y, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.ones(shape=(10,300), dtype=tf.float32)\n",
    "x = tf.ones(shape=(10,300), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_2 (encoder)         multiple                  19055616  \n",
      "                                                                 \n",
      " decoder_2 (decoder)         multiple                  25506816  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44562432 (169.99 MB)\n",
      "Trainable params: 44562432 (169.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t = Transformer(input_dims= 300, output_dims=300)\n",
    "z = t(x, y)\n",
    "t.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
